# -*- coding: utf-8 -*-
"""posture-ai

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QjKoZsqBAKG-D3ebwFQ30iNFRUzrhdzV
"""

!pip install mediapipe opencv-python numpy

from IPython.display import Javascript

# Iniciar la webcam en la pantalla
js = Javascript("""
async function startWebcam() {
    const div = document.createElement('div');
    document.body.appendChild(div);

    const video = document.createElement('video');
    video.style.display = 'block';
    video.width = 480;
    video.height = 360;
    div.appendChild(video);

    const stream = await navigator.mediaDevices.getUserMedia({video: true});
    video.srcObject = stream;
    await video.play();
}
startWebcam();
""")

display(js)

from google.colab import output
from IPython.display import Javascript
import base64

photo_data = ""

# Funci√≥n Python que recibe la imagen desde JS
def _receive_image(data):
    global photo_data
    photo_data = data

output.register_callback('notebook.receiveImage', _receive_image)


def take_photo(filename="photo.jpg", quality=0.8):
    js = Javascript(f"""
    async function takePhoto() {{
        const video = document.querySelector('video');

        // Crear canvas
        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;

        // Dibujar frame actual
        canvas.getContext('2d').drawImage(video, 0, 0);

        // Obtener imagen
        const dataUrl = canvas.toDataURL('image/jpeg', {quality});

        // Enviar a Python
        google.colab.kernel.invokeFunction(
            'notebook.receiveImage',
            [dataUrl],
            {{}}
        );
    }}

    takePhoto();
    """)

    display(js)

    # Esperar a que JS env√≠e la foto
    import time
    while photo_data == "":
        time.sleep(0.1)

    # Guardar archivo
    header, encoded = photo_data.split(",", 1)
    img = base64.b64decode(encoded)

    with open(filename, "wb") as f:
        f.write(img)

    return filename

import cv2
import numpy as np
import mediapipe as mp
from google.colab.patches import cv2_imshow

mp_face_mesh = mp.solutions.face_mesh
mp_draw = mp.solutions.drawing_utils

LEFT_EYE = [33, 160, 158, 133, 153, 144]
RIGHT_EYE = [263, 387, 385, 362, 380, 373]


def eye_aspect_ratio(landmarks, eye_idx):
    pts = np.array([[landmarks[i].x, landmarks[i].y] for i in eye_idx])

    dist1 = np.linalg.norm(pts[1] - pts[5])
    dist2 = np.linalg.norm(pts[2] - pts[4])
    dist3 = np.linalg.norm(pts[0] - pts[3])

    return (dist1 + dist2) / (2.0 * dist3)


# Capturar imagen
filename = take_photo()

# Cargar
frame = cv2.imread(filename)
rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

# Procesar
with mp_face_mesh.FaceMesh(static_image_mode=True) as face_mesh:
    res = face_mesh.process(rgb)

if not res.multi_face_landmarks:
    print("‚ùå No se detect√≥ rostro")
else:
    lm = res.multi_face_landmarks[0].landmark

    left_EAR  = eye_aspect_ratio(lm, LEFT_EYE)
    right_EAR = eye_aspect_ratio(lm, RIGHT_EYE)
    ear_avg   = (left_EAR + right_EAR) / 2

    print("üëÅ EAR Izquierda:", left_EAR)
    print("üëÅ EAR Derecha:", right_EAR)
    print("üëâ EAR Promedio:", ear_avg)

    annotated = frame.copy()
    mp_draw.draw_landmarks(
        annotated,
        res.multi_face_landmarks[0],
        mp_face_mesh.FACEMESH_CONTOURS
    )

    cv2_imshow(annotated)

import time

EAR_THRESHOLD = 0.21        # EAR menor ‚Üí ojo cerrado
CONSEC_FRAMES = 2           # Frames consecutivos con ojo cerrado
NO_BLINK_SECONDS = 10       # Tiempo sin parpadeos = fatiga

blink_counter = 0
total_blinks = 0
last_blink_time = time.time()
fatigue_alert = False

def process_frame_for_blink(frame, results, landmarks):
    global blink_counter, total_blinks, last_blink_time, fatigue_alert

    left_EAR = eye_aspect_ratio(landmarks, LEFT_EYE)
    right_EAR = eye_aspect_ratio(landmarks, RIGHT_EYE)
    avg_EAR = (left_EAR + right_EAR) / 2

    eye_closed = avg_EAR < EAR_THRESHOLD

    # 1. Detectar si el ojo est√° cerrado
    if eye_closed:
        blink_counter += 1
    else:
        # Detectar parpadeo SOLO cuando el ojo se vuelve a abrir
        if blink_counter >= CONSEC_FRAMES:
            total_blinks += 1
            last_blink_time = time.time()
            print("üü¢ Parpadeo detectado. Total:", total_blinks)
        blink_counter = 0

    # 2. Detectar fatiga visual
    time_since_blink = time.time() - last_blink_time
    fatigue_alert = time_since_blink >= NO_BLINK_SECONDS

    if fatigue_alert:
        print("‚ö†Ô∏è FATIGA VISUAL: llevas m√°s de", NO_BLINK_SECONDS, "segundos sin parpadear.")

    return avg_EAR, fatigue_alert

filename = take_photo()
frame = cv2.imread(filename)
rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

with mp_face_mesh.FaceMesh(static_image_mode=True) as face_mesh:
    res = face_mesh.process(rgb)

if not res.multi_face_landmarks:
    print("‚ùå No se detect√≥ rostro")
else:
    landmarks = res.multi_face_landmarks[0].landmark

    avg_EAR, fatigue = process_frame_for_blink(frame, res, landmarks)

    print("EAR Promedio:", avg_EAR)
    print("Parpadeos detectados:", total_blinks)
    print("Fatiga visual:", "S√≠" if fatigue else "No")

    annotated = frame.copy()
    mp_draw.draw_landmarks(
        annotated,
        res.multi_face_landmarks[0],
        mp_face_mesh.FACEMESH_CONTOURS
    )
    cv2_imshow(annotated)

# =========================================================================
# Detecci√≥n de Postura
# =========================================================================

import mediapipe as mp
import numpy as np
import math

mp_pose = mp.solutions.pose
mp_drawing = mp.solutions.drawing_utils

# Funci√≥n auxiliar para calcular el √°ngulo entre 3 puntos (landmarks)
def calculate_angle(a, b, c):
    """Calcula el √°ngulo en grados entre los puntos A, B y C (donde B es el v√©rtice)"""
    a = np.array(a) # Primer punto (e.g., Hombro Izquierdo)
    b = np.array(b) # V√©rtice (e.g., Nariz/Cuello)
    c = np.array(c) # Tercer punto (e.g., Hombro Derecho)

    # Convertir a coordenadas relativas y calcular el √°ngulo
    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])
    angle = np.abs(radians * 180.0 / np.pi)

    # Asegurar que el √°ngulo est√© en el rango [0, 180]
    if angle > 180.0:
        angle = 360 - angle

    return angle

# --- Variables de Postura ---
# Un √°ngulo de 180 grados indica una l√≠nea recta perfecta (espalda recta o cuello centrado).
# Un √°ngulo de 160-175 grados puede indicar una leve inclinaci√≥n hacia adelante (mala postura).
POSTURE_THRESHOLD = 170 # Si el √°ngulo es menor a esto, se considera mala postura

def process_frame_for_posture(landmarks):
    """Procesa los landmarks para detectar una mala postura (inclinaci√≥n de cabeza)"""

    # Obtener las coordenadas X, Y de los landmarks de inter√©s
    # Usaremos los hombros y la nariz (o el punto medio del cuello) para detectar la inclinaci√≥n de la cabeza
    left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,
                     landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]
    right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,
                      landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]
    nose = [landmarks[mp_pose.PoseLandmark.NOSE.value].x,
            landmarks[mp_pose.PoseLandmark.NOSE.value].y]

    # Calcular el √°ngulo formado por los hombros y la nariz.
    # Un √°ngulo menor a 180 indica que la cabeza est√° inclinada hacia adelante.
    head_alignment_angle = calculate_angle(left_shoulder, nose, right_shoulder)

    # Determinar si la postura es mala
    is_bad_posture = head_alignment_angle < POSTURE_THRESHOLD

    return head_alignment_angle, is_bad_posture

# --- Simulaci√≥n de una Detecci√≥n de Postura ---

# Capturar una nueva imagen para el an√°lisis de Postura
print("\n--- Iniciando Detecci√≥n de Postura ---")
filename_posture = take_photo(filename="photo_posture.jpg")
frame_posture = cv2.imread(filename_posture)
rgb_posture = cv2.cvtColor(frame_posture, cv2.COLOR_BGR2RGB)

# Procesar con MediaPipe Pose
with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5) as pose:
    res_pose = pose.process(rgb_posture)

if not res_pose.pose_landmarks:
    print("‚ùå No se detect√≥ postura corporal.")
else:
    landmarks_pose = res_pose.pose_landmarks.landmark

    angle_posture, bad_posture = process_frame_for_posture(landmarks_pose)

    print("üìè √Ångulo de Alineaci√≥n (Hombros-Nariz):", angle_posture)
    if bad_posture:
        print(f"‚ö†Ô∏è MALA POSTURA: El √°ngulo ({angle_posture:.2f}) es menor al umbral ({POSTURE_THRESHOLD}).")
        print("üëâ ¬°Notificaci√≥n! Por favor, endereza la espalda y alinea la cabeza.")
    else:
        print("‚úÖ Postura Correcta.")

    # Dibujar los landmarks de la postura para visualizaci√≥n
    annotated_posture = frame_posture.copy()
    mp_drawing.draw_landmarks(
        annotated_posture,
        res_pose.pose_landmarks,
        mp_pose.POSE_CONNECTIONS,
        mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),
        mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)
    )
    cv2_imshow(annotated_posture)

# =========================================================================
# Bucle Unificado para Simulaci√≥n en Tiempo Real
# NOTA: Este c√≥digo simula un loop capturando una foto cada 2 segundos.
# En una aplicaci√≥n real, se procesar√≠a un frame de video continuo.
# =========================================================================

# Inicializaci√≥n de modelos (MediaPipe Face Mesh y Pose)
mp_face_mesh = mp.solutions.face_mesh
mp_pose = mp.solutions.pose
mp_draw = mp.solutions.drawing_utils

# Variables globales ya definidas, aseg√∫rate que est√©n inicializadas antes:
# blink_counter = 0
# total_blinks = 0
# last_blink_time = time.time()
# fatigue_alert = False

NUM_FRAMES_TO_PROCESS = 10 # N√∫mero de veces que el loop simular√° una captura/proceso

print("--- üö¶ Iniciando An√°lisis Combinado (Simulaci√≥n en Loop) üö¶ ---")
print(f"Procesando {NUM_FRAMES_TO_PROCESS} 'frames' (capturas) con un intervalo de 2 segundos...")

# Inicializa los objetos de MediaPipe para su uso dentro del loop
with mp_face_mesh.FaceMesh(static_image_mode=True) as face_mesh, \
     mp_pose.Pose(static_image_mode=True) as pose:

    for i in range(NUM_FRAMES_TO_PROCESS):

        # 1. Capturar la imagen (simula la toma de un frame de video)
        time.sleep(2) # Pausa para simular un intervalo de tiempo (ej. 2 segundos)
        print(f"\n--- FRAME {i + 1}/{NUM_FRAMES_TO_PROCESS} ---")

        filename = take_photo(filename=f"frame_{i}.jpg")
        frame = cv2.imread(filename)
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # 2. Procesamiento de Postura (MediaPipe Pose)
        res_pose = pose.process(rgb)

        if res_pose.pose_landmarks:
            landmarks_pose = res_pose.pose_landmarks.landmark
            angle_posture, bad_posture = process_frame_for_posture(landmarks_pose)

            if bad_posture:
                print(f"‚ö†Ô∏è POSTURA: √Ångulo {angle_posture:.2f}¬∞ -> ¬°Notificaci√≥n! Endereza la espalda.")
            else:
                print(f"‚úÖ POSTURA: Correcta ({angle_posture:.2f}¬∞).")
        else:
            print("‚ùå POSTURA: No se detect√≥ cuerpo completo.")


        # 3. Procesamiento de Fatiga Visual (MediaPipe Face Mesh / EAR)
        res_face = face_mesh.process(rgb)

        if res_face.multi_face_landmarks:
            landmarks_face = res_face.multi_face_landmarks[0].landmark
            avg_EAR, fatigue = process_frame_for_blink(frame, res_face, landmarks_face)

            print(f"üëÅ FATIGA: EAR Promedio {avg_EAR:.4f}")

            if fatigue:
                # La funci√≥n process_frame_for_blink ya imprime la alerta de fatiga, pero la reforzamos:
                print("üö® ALERTA COMBINADA: ¬°Toma un descanso de la pantalla!")

        else:
            print("‚ùå FATIGA: No se detect√≥ rostro.")

        # Opcional: Mostrar el frame (con dibujos de postura) solo para el √∫ltimo frame
        if i == NUM_FRAMES_TO_PROCESS - 1 and res_pose.pose_landmarks:
             annotated = frame.copy()
             mp_draw.draw_landmarks(
                 annotated,
                 res_pose.pose_landmarks,
                 mp_pose.POSE_CONNECTIONS
             )
             cv2_imshow(annotated)


print("\n--- ‚úÖ An√°lisis Combinado Finalizado ---")
print("Estad√≠sticas Finales:")
print(f"Total de Parpadeos Detectados: {total_blinks}")
print(f"Tiempo sin Parpadear al Final: {time.time() - last_blink_time:.2f} segundos")